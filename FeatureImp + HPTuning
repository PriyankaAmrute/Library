import streamlit as st
import os
import pandas as pd
import numpy as np
from scipy.signal import savgol_filter
import tsfel
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.svm import SVC, SVR
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBClassifier, XGBRegressor
import plotly.express as px
import plotly.graph_objects as go
import re
import streamlit.components.v1 as components



def display_classification_report(accuracy, report, model_name=None):
    model_info = f"<h2 style='color: #000000; text-align: center; font-family: Arial, sans-serif;'>{model_name}</h2>" if model_name else ""
    report_html = f"""
    <div style="border: 1px solid #E0E0E0; border-radius: 10px; padding: 20px; background-color: #FAFAFA; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2); font-family: Arial, sans-serif;">
        {model_info}
        <h3 style='color: #000000; text-align: center;'>Accuracy: {accuracy:.2f}</h3>
        <h4 style='color: ##000000;'>Report:</h4>
        <pre style="font-size: small; background-color: #FFFFFF; padding: 15px; border-radius: 5px; overflow: auto; white-space: pre-wrap; border: 1px solid #E0E0E0;">{report}</pre>
    </div>
    """
    components.html(report_html, height=430, scrolling=True)

    
    
def display_regression_metrics(mse, r2, model_name=None):
    model_info = f"<h2 style='color: #000000; text-align: center; font-family: Arial, sans-serif;'>{model_name}</h2>" if model_name else ""
    metrics_html = f"""
    <div style="border: 1px solid #E0E0E0; border-radius: 10px; padding: 20px; background-color: #FAFAFA; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2); font-family: Arial, sans-serif;">
        {model_info}
        <h3 style='color: #000000; text-align: center;'>MSE: {mse:.2f}</h3>
        <h3 style='color: #000000; text-align: center;'>R2 Score: {r2:.2f}</h3>
    </div>
    """
    components.html(metrics_html, height=250, scrolling=True)



def load_data_from_folder(folder_path):
    data = []
    for subdir, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(subdir, file)
                df = pd.read_csv(file_path, header=None)
                df['filename'] = file
                data.append(df)
    data = pd.DataFrame(data)
    return data



def count_xls_files(folder_path):
    xls_count = 0
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.csv'):
                xls_count += 1
                file_path = os.path.join(root, file)
    return xls_count



def df_maker_smooth(path, window_size, poly_order, target_expression):
    all_features_df = pd.DataFrame()
    for subfolder in os.listdir(path):
        subfolder_path = os.path.join(path, subfolder)
        if os.path.isdir(subfolder_path):
            for file in os.listdir(subfolder_path):
                if file.endswith(".csv"):
                    file_path = os.path.join(subfolder_path, file)
                    df = pd.read_csv(file_path)
                    df['filename'] = file
                    signal = df.iloc[:, 0].values
                    smoothened_signal = golay_filter(signal, window_size, poly_order)
                    smoothened_signal_df = pd.DataFrame(smoothened_signal)
                    df['target'] = df['filename'].str.extract(f'({target_expression})(\d+)')[1].astype(float)
                    cfg = tsfel.get_features_by_domain()
                    features = tsfel.time_series_features_extractor(cfg, smoothened_signal_df.iloc[:, 0], 1000000)
                    features_transposed = features.T.reset_index(drop=True).T
                    features_transposed['target'] = df['target']
                    all_features_df = pd.concat([all_features_df, features_transposed], ignore_index=True)
    return all_features_df



def df_maker_unsmooth(path, target_expression):
    all_features_df = pd.DataFrame()
    for subfolder in os.listdir(path):
        subfolder_path = os.path.join(path, subfolder)
        if os.path.isdir(subfolder_path):
            for file in os.listdir(subfolder_path):
                if file.endswith(".csv"):
                    file_path = os.path.join(subfolder_path, file)
                    df = pd.read_csv(file_path)
                    df['filename'] = file
                    signal = df.iloc[:, 0].values
                    df['target'] = df['filename'].str.extract(f'({target_expression})(\d+)')[1].astype(float)
                    cfg = tsfel.get_features_by_domain()
                    features = tsfel.time_series_features_extractor(cfg, df.iloc[:, 0], 1000000)
                    features_transposed = features.T.reset_index(drop=True).T
                    features_transposed['target'] = df['target']
                    all_features_df = pd.concat([all_features_df, features_transposed], ignore_index=True)
    return all_features_df



def reduce_features(features, threshold):
    corr_matrix = features.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    reduced_features = features.drop(columns=to_drop)
    return reduced_features



def golay_filter(signal, window_size, poly_order):
    return savgol_filter(signal, window_length=window_size, polyorder=poly_order)



def scale_data(data, scaler_type):
    if scaler_type == "MinMax":
        scaler = MinMaxScaler()
    else:
        scaler = StandardScaler()
    scaled_data = pd.DataFrame(scaler.fit_transform(data.drop(columns=['target'])), columns=data.columns[:-1])
    scaled_data['target'] = data['target']
    return scaled_data, scaler



def split(scaled_data, test_size):
    X = scaled_data.drop(columns=['target'])
    y = scaled_data['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    return X_train, X_test, y_train, y_test



def models(learning_type, selected_model, X_train, X_test, y_train, y_test):
    if learning_type == "Classification":
        models = {
            "Random Forest Classifier": RandomForestClassifier(),
            "Gradient Boosting Classifier": GradientBoostingClassifier(),
            "AdaBoost Classifier": AdaBoostClassifier(),
            "Decision Tree Classifier": DecisionTreeClassifier(),
            "SVC": SVC()
        }
        if selected_model != "All Models":
            model = models[selected_model]
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            report = classification_report(y_test, y_pred)
            return accuracy, report, model
        else:
            results = {}
            reports = {}
            trained_models = {}
            for name, model in models.items():
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                report = classification_report(y_test, y_pred)
                results[name] = accuracy
                reports[name] = report
                trained_models[name] = model
            return results, reports, trained_models
    else:
        models = {
            "Random Forest Regressor": RandomForestRegressor(),
            "Gradient Boosting Regressor": GradientBoostingRegressor(),
            "Decision Tree Regressor": DecisionTreeRegressor(),
            "XGBoost Regressor": XGBRegressor(),
            "SVR": SVR()
        }
        if selected_model != "All Models":
            model = models[selected_model]
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            return mse, r2, model
        else:
            results = {}
            reports = {}
            trained_models = {}
            for name, model in models.items():
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                results[name] = (mse, r2)
                trained_models[name] = model
            return results, reports, trained_models



def plot_classification_accuracy(results):
    accuracy_df = pd.DataFrame(columns=['Accuracy'])
    for model_name, accuracy in results.items():
        accuracy_df.loc[model_name] = [accuracy]

    st.write("Model Performance Comparison (Classification):")
    st.write(accuracy_df)

    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=accuracy_df.index,
        y=accuracy_df['Accuracy'],
        name='Accuracy',
        marker_color='green'
    ))
    fig.update_layout(
        xaxis_tickangle=-45,
        title='Model Performance Comparison (Classification)',
        xaxis_title='Model',
        yaxis_title='Accuracy'
    )
    st.plotly_chart(fig)

    
    
def plot_regression_metrics(results):
    mse_r2_df = pd.DataFrame(columns=['MSE', 'R2 Score'])
    for model_name, scores in results.items():
        mse, r2 = scores
        mse_r2_df.loc[model_name] = [mse, r2]

    st.write("Model Performance Comparison (Regression):")
    st.write(mse_r2_df)

    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=mse_r2_df.index,
        y=mse_r2_df['MSE'],
        name='MSE',
        marker_color='blue'
    ))
    fig.add_trace(go.Bar(
        x=mse_r2_df.index,
        y=mse_r2_df['R2 Score'],
        name='R2 Score',
        marker_color='orange'
    ))
    fig.update_layout(
        barmode='group',
        xaxis_tickangle=-45,
        title='Model Performance Comparison (Regression)',
        xaxis_title='Model',
        yaxis_title='Value'
    )
    st.plotly_chart(fig)


    
def display_feature_importance(model, X_train):
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        feature_names = X_train.columns
        feature_importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': importances
        }).sort_values(by='Importance', ascending=False)
        
        fig = go.Figure([go.Bar(x=feature_importance_df['Feature'], y=feature_importance_df['Importance'])])
        fig.update_layout(title='Feature Importance', xaxis_title='Feature', yaxis_title='Importance')
        
        st.plotly_chart(fig)
    else:
        st.warning("Selected model does not support feature importance.")


        
def main():
    st.title("CT-PIU Gap Measurement")

    if "score1" not in st.session_state:
        st.session_state.score1 = None
    if "score2" not in st.session_state:
        st.session_state.score2 = None
    if "model" not in st.session_state:
        st.session_state.model = None
    if "result" not in st.session_state:
        st.session_state.result = None
    if "reports" not in st.session_state:
        st.session_state.reports = None
    if "xls_count" not in st.session_state:
        st.session_state.xls_count = None
    if "target_expression" not in st.session_state:
        st.session_state.target_expression = None
    if "data_loaded" not in st.session_state:
        st.session_state.data_loaded = False

    folder_path = st.text_input("Enter the path to the main folder")

    if folder_path:
        if st.button("Count .xls Files and Calculate Max Values"):
            if os.path.isdir(folder_path):
                xls_count = count_xls_files(folder_path)
                st.session_state.xls_count = xls_count
                st.success(f"Total number of .xls files: {xls_count}")

    if st.session_state.xls_count is not None:
        target_expression = st.text_input("Enter the target expression (e.g., __OD__)", "_OD_")
        if st.button("Load Data"):
            st.session_state.target_expression = target_expression
            st.session_state.data_loaded = True

    if st.session_state.data_loaded:
        remove_noise = st.sidebar.radio("Remove Noise and Feature Extraction", ('Yes', 'No'))
        if remove_noise == "Yes":
            window_size = st.sidebar.slider("Window Size", 3, 51, step=2, value=5)
            poly_order = st.sidebar.slider("Polynomial Order", 1, 5, value=2)
            if "smooth_df" not in st.session_state:
                with st.spinner("Processing..."):
                    st.session_state.smooth_df = df_maker_smooth(folder_path, window_size, poly_order, st.session_state.target_expression)
            df = st.session_state.smooth_df
            st.write("Noise removed using Savitzky-Golay filter.")
            st.write(df)
            st.write("Shape", df.shape)
        else:
            if "unsmooth_df" not in st.session_state:
                with st.spinner("Processing..."):
                    st.session_state.unsmooth_df = df_maker_unsmooth(folder_path, st.session_state.target_expression)
            df = st.session_state.unsmooth_df
            st.write("Using raw data.")
            st.write(df)
            st.write("Shape", df.shape)

        threshold = st.sidebar.slider("Correlation Threshold", 0.0, 1.0, 0.9)
        if st.sidebar.button("Correlation"):
            st.session_state.reduced_features = reduce_features(df, threshold)
            reduced_features = st.session_state.reduced_features
            st.write("Reduced features based on correlation.", reduced_features)
            st.write("Shape", reduced_features.shape)
        else:
            if "reduced_features" in st.session_state:
                reduced_features = st.session_state.reduced_features
            else:
                reduced_features = df

        scale_data_flag = st.sidebar.checkbox("Scale Data", value=False)
        if scale_data_flag:
            st.write(st.session_state.reduced_features)
            scaler_type = st.sidebar.selectbox("Scaler Type", ["Standard", "MinMax"])
            scaled_data, scaler = scale_data(reduced_features, scaler_type)
            st.write("Data scaled using", scaler_type, "scaler.", scaled_data)
        else:
            scaled_data = reduced_features

        learning_type = st.sidebar.selectbox("Learning Type", ["Classification", "Regression"])
        test_size = st.sidebar.slider("Test Size Split Ratio", 0.1, 0.5, 0.2)

        if st.sidebar.button("Split"):
            X_train, X_test, y_train, y_test = split(scaled_data, test_size)
            st.session_state.X_train = X_train
            st.session_state.X_test = X_test
            st.session_state.y_train = y_train
            st.session_state.y_test = y_test
            st.write(st.session_state.X_train.shape)

        if learning_type == "Classification":
            models_list = [
                "Random Forest Classifier",
                "Gradient Boosting Classifier",
                "AdaBoost Classifier",
                "Decision Tree Classifier",
                "SVC"
            ]
        else:
            models_list = [
                "Random Forest Regressor",
                "Gradient Boosting Regressor",
                "Decision Tree Regressor",
                "XGBoost Regressor",
                "SVR"
            ]

        selected_model = st.sidebar.selectbox("Select Model", models_list + ["All Models"])

        if st.sidebar.button("Train"):
            if "X_train" in st.session_state and "X_test" in st.session_state and "y_train" in st.session_state and "y_test" in st.session_state:
                if selected_model == "All Models" and learning_type == "Classification":
                    st.session_state.result, st.session_state.reports, trained_models = models(
                        learning_type, selected_model, st.session_state.X_train, st.session_state.X_test,
                        st.session_state.y_train, st.session_state.y_test)
                    plot_classification_accuracy(st.session_state.result)
                    for model_name, report in st.session_state.reports.items():
                        display_classification_report(st.session_state.result[model_name], report, model_name)
                elif selected_model == "All Models" and learning_type == "Regression":
                    st.session_state.result, _, trained_models = models(
                        learning_type, selected_model, st.session_state.X_train, st.session_state.X_test,
                        st.session_state.y_train, st.session_state.y_test)
                    plot_regression_metrics(st.session_state.result)
                else:
                    st.session_state.score1, st.session_state.score2, st.session_state.model = models(
                        learning_type, selected_model, st.session_state.X_train, st.session_state.X_test,
                        st.session_state.y_train, st.session_state.y_test)
                    if learning_type == "Regression":
                        display_regression_metrics(st.session_state.score1, st.session_state.score2)
                    else:
                        display_classification_report(st.session_state.score1, st.session_state.score2)
            else:
                st.error("Please split the data before training the model.")

#         if selected_model != "All Models" and st.sidebar.button("Show Feature Importance"):
#             if "model" in st.session_state:
#                 display_feature_importance(st.session_state.model, st.session_state.X_train)
#             else:
#                 st.error("Train a single model first to see feature importance.")
                
        if selected_model != "All Models" and st.sidebar.button("Show Feature Importance"):
            if "model" in st.session_state:
                display_feature_importance(st.session_state.model, st.session_state.X_train)
                top_n_features = st.sidebar.number_input("Select Top N Features", min_value=1, max_value=len(st.session_state.X_train.columns), value=5)
                top_features = st.session_state.X_train.columns[np.argsort(-st.session_state.model.feature_importances_)[:top_n_features]]

                method = st.sidebar.selectbox("Hyperparameter Tuning Method", ["Randomized Search", "Grid Search"])
                if method == "Randomized Search":
                    # Define hyperparameter grid for randomized search
                    param_grid = {
                        'n_estimators': [50, 100, 200, 300],
                        'max_depth': [3, 5, 10, None],
                        'min_samples_split': [2, 5, 10],
                        'min_samples_leaf': [1, 2, 4],
                        'bootstrap': [True, False]
                    }
                    tuner = RandomizedSearchCV(estimator=st.session_state.model, param_distributions=param_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)
                else:
                    # Define hyperparameter grid for grid search
                    param_grid = {
                        'n_estimators': [50, 100, 200, 300],
                        'max_depth': [3, 5, 10, None],
                        'min_samples_split': [2, 5, 10],
                        'min_samples_leaf': [1, 2, 4],
                        'bootstrap': [True, False]
                    }
                    tuner = GridSearchCV(estimator=st.session_state.model, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1)

                if st.sidebar.button("Train After Hyperparameter Tuning"):
                    with st.spinner("Performing Hyperparameter Tuning..."):
                        tuner.fit(st.session_state.X_train[top_features], st.session_state.y_train)

                    st.session_state.model = tuner.best_estimator_
                    st.session_state.score1, st.session_state.score2, _ = models(learning_type, selected_model, st.session_state.X_train[top_features], st.session_state.X_test[top_features], st.session_state.y_train, st.session_state.y_test)

                    if learning_type == "Regression":
                        display_regression_metrics(st.session_state.score1, st.session_state.score2)
                        # Plot actual vs predicted values
                        fig = px.bar(x=st.session_state.y_test.index, y=st.session_state.y_test.values, labels={'x':'Index', 'y':'Value'}, title='Actual vs Predicted Values')
                        fig.add_bar(x=st.session_state.y_test.index, y=st.session_state.model.predict(st.session_state.X_test[top_features]), name='Predicted Values')
                        st.plotly_chart(fig)

                    else:
                        display_classification_report(st.session_state.score1, st.session_state.score2)
                        # Plot actual vs predicted values
                        fig = px.bar(x=st.session_state.y_test.index, y=st.session_state.y_test.values, labels={'x':'Index', 'y':'Value'}, title='Actual vs Predicted Values')
                        fig.add_bar(x=st.session_state.y_test.index, y=st.session_state.model.predict(st.session_state.X_test[top_features]), name='Predicted Values')
                        st.plotly_chart(fig)

                else:
                    st.warning("Perform Hyperparameter Tuning and click Train After Hyperparameter Tuning.")
            else:
                st.error("Train a single model first to see feature importance.")

if __name__ == "__main__":
    main()
